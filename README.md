<div align="center">
  <!-- Banner -->
  <img src="assets/github-header-banner.png" width="100%" alt="Kael Valen Banner" />

  <br/><br/>

  <!-- Avatar -->
  <a href="https://github.com/kaelvalen">
    <img src="https://avatars.githubusercontent.com/u/131282394?v=4" width="140" style="border-radius:50%; border:3px solid #7f5af0;" alt="Kael Valen Avatar" />
  </a>

  <h1>Arda Hakbilen (<strong>Kael Valen</strong>)</h1>
  <p><em>ML Architecture Researcher · Non-Transformer Models · Systematic Learning</em></p>

  <!-- Typing SVG -->
  <br/>
  <a href="https://git.io/typing-svg">
    <img src="https://readme-typing-svg.demolab.com?font=Fira+Code&weight=600&size=26&duration=3500&pause=800&color=7F5AF0&center=true&vCenter=true&multiline=true&repeat=true&random=false&width=850&height=180&lines=Building+Beyond+Transformer;Exploring+State-Space+Models+%26+Hybrid+RNNs;+%7C+Phase+1%2F6;Questioning+Transformer+Monopoly" alt="Typing SVG" />
  </a>
  <br/>
  <br/>
  <br/>

  <!-- Focused Badges -->
  <p>
    <img src="https://img.shields.io/badge/Research-Alternative%20Architectures-7f5af0?style=for-the-badge&logo=openai&logoColor=white" alt="Research badge" />
    <img src="https://img.shields.io/badge/Focus-Beyond%20Transformer-fd6ff8?style=for-the-badge&logo=pytorch&logoColor=white" alt="Focus badge" />
    <img src="https://komarev.com/ghpvc/?username=kaelvalen&label=Profile%20views&color=7f5af0&style=for-the-badge" alt="Profile views counter" />
  </p>

  <br/>

  <!-- Research Statement -->
  <p align="center" style="max-width: 800px; margin: 0 auto;">
    <em>
      Researching <strong>non-transformer architectures</strong> through systematic implementation.<br/>
      Not just reading papers — building Mamba, RWKV, Flash Attention from scratch.<br/>
    </em>
  </p>

  <br/>

  <!-- Socials -->
  <p>
    <a href="mailto:mehmetardahakbilen2005@gmail.com"><img src="https://img.shields.io/badge/Email-Contact%20Me-e63946?style=for-the-badge&logo=gmail&logoColor=white" alt="Email" /></a>
    <a href="https://www.linkedin.com/in/mehmet-arda-hakbilen-12aba6269/"><img src="https://img.shields.io/badge/LinkedIn-Connect-0a66c2?style=for-the-badge&logo=linkedin&logoColor=white" alt="LinkedIn" /></a>
    <a href="https://github.com/kaelvalen"><img src="https://img.shields.io/badge/GitHub-@kaelvalen-222?style=for-the-badge&logo=github&logoColor=white" alt="GitHub" /></a>
  </p>
</div>

---

<div align="center">
  <h2>GitHub Analytics & Activity</h2>
  <p><em>Tracking systematic learning through daily commits</em></p>
  <br>

  <br/><br/>

<!-- 3D Contribution Graph -->
<div align="center">
  <h2>3D Contribution Graph</h2>
  <img
    src="https://raw.githubusercontent.com/kaelvalen/kaelvalen/output/profile-night-view.svg"
    alt="3D Contribution Graph"
    width="100%"
  />
</div>
  <!-- Generated by .github/workflows/metrics.yml -->
  <img src="https://github.com/kaelvalen/kaelvalen/blob/metrics/github-metrics.svg" alt="Kael Valen's Metrics" width="85%" />
</div>

<br />

<!-- Snake Animation Placeholder (Will be generated by GitHub Action) -->
<div align="center">
  <img src="https://github.com/kaelvalen/kaelvalen/blob/output/github-contribution-grid-snake.svg" alt="Snake Animation" width="100%" />
</div>

<br/>


---

## Research & Core Focus

### **Beyond Transformer: Alternative Sequence Architectures**
> Questioning the assumption that transformers are the only viable solution

-   **State-Space Models** → Mamba & RWKV implementations
    -   Linear-time inference vs quadratic attention complexity
    -   Selective state mechanisms for efficient memory
    -   Comparing trade-offs: speed vs expressiveness

-   **Hybrid Architectures** → RNN + Attention combinations
    -   Exploring best of both worlds: recurrence + selectivity
    -   Custom memory systems for long-context tasks
    -   Implementation-first approach to understanding

-   **Flash Attention v2** → From-scratch CUDA optimization
    -   Understanding memory-efficient attention at kernel level
    -   Production inference optimization
    -   10x speedup through proper memory access patterns

---

## Featured Projects

<table>
  <tr>
    <td width="50%">
      <h3>Beyond Transformer</h3>
      <p><strong>Open-source alternative architecture research</strong></p>
      <p>
        Implementation-first approach to understanding non-transformer models. Building Mamba, RWKV, and hybrid systems from scratch to compare trade-offs.
      </p>
      <p>
        <code>PyTorch</code> <code>JAX</code> <code>CUDA</code> <code>ONNX</code>
      </p>
      <p>
        <a href="https://github.com/kaelvalen/beyond_transformer">
          <img src="https://img.shields.io/badge/View_Project-7f5af0?style=for-the-badge&logo=github" />
        </a>
      </p>
      <p>
        <strong>Current Phase:</strong><br/>
        • Flash Attention v2 from scratch<br/>
        • Mamba state-space model analysis<br/>
        • RWKV architecture comparison
      </p>
    </td>
    <td width="50%">
      <h3>Learning Tracker</h3>
      <p><strong>Structured habit & progress management</strong></p>
      <p>
        SQLite-based system for breaking vibe coding habits. Daily plans, evening reports, focus zone management, and streak tracking with AI coach integration.
      </p>
      <p>
        <code>Python</code> <code>SQLite</code> <code>CLI</code> <code>Anthropic API</code>
      </p>
      <p>
        <img src="https://img.shields.io/badge/Status-Active_Use-success?style=for-the-badge" />
      </p>
      <p>
        <strong>Features:</strong><br/>
        • GREEN/RED focus zone tracking<br/>
        • Morning planning + evening review<br/>
        • AI coach for accountability
      </p>
    </td>
  </tr>
  <tr>
    <td width="50%">
      <h3>SentinelFS</h3>
      <p><strong>Distributed P2P File Sync (Archived)</strong></p>
      <p>
        Autonomous peer-to-peer synchronization with ML-based anomaly detection, delta-sync algorithms, and genetic topology remeshing for fault tolerance.
      </p>
      <p>
        <code>C++17</code> <code>Threading</code> <code>P2P</code> <code>ML</code>
      </p>
      <p>
        <a href="https://github.com/kaelvalen/SentinelFS">
          <img src="https://img.shields.io/badge/View_Project-fd6ff8?style=for-the-badge&logo=github" />
        </a>
      </p>
      <p>
        <strong>Key Features:</strong><br/>
        • ML anomaly detection pipeline<br/>
        • Self-healing network topology<br/>
        • Zero-copy delta sync protocol<br/>
        • Byzantine fault tolerance
      </p>
    </td>
    <td width="50%">
      <h3>Research Blog (Planned)</h3>
      <p><strong>Implementation Notes & Architecture Analysis</strong></p>
      <p>
        Documenting learning journey: from-scratch implementations, architecture comparisons, and trade-off analysis. Focus on practical insights over theory.
      </p>
      <p>
        <code>Markdown</code> <code>GitHub Pages</code> <code>Technical Writing</code>
      </p>
      <p>
        <img src="https://img.shields.io/badge/Status-Q2_2025-orange?style=for-the-badge" />
      </p>
      <p>
        <strong>Planned Topics:</strong><br/>
        • Mamba vs Transformer trade-offs<br/>
        • Flash Attention internals<br/>
        • CUDA kernel optimization<br/>
        • JAX vs PyTorch for research
      </p>
    </td>
  </tr>
</table>

---

## Tech Stack & Tools

<div align="center">
  <table>
    <tr>
      <td align="center" width="140"><strong>ML Frameworks</strong></td>
      <td>
        <img src="https://img.shields.io/badge/PyTorch-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white" />
        <img src="https://img.shields.io/badge/CUDA-76B900?style=for-the-badge&logo=nvidia&logoColor=white" />
        <img src="https://img.shields.io/badge/JAX-00599C?style=for-the-badge&logo=google&logoColor=white" />
        <img src="https://img.shields.io/badge/ONNX-005CED?style=for-the-badge&logo=onnx&logoColor=white" />
      </td>
    </tr>
    <tr>
      <td align="center" width="140"><strong>Research Tools</strong></td>
      <td>
        <img src="https://img.shields.io/badge/HuggingFace-FFD21E?style=for-the-badge&logo=huggingface&logoColor=black" />
        <img src="https://img.shields.io/badge/W&B-FFBE00?style=for-the-badge&logo=weightsandbiases&logoColor=black" />
        <img src="https://img.shields.io/badge/Jupyter-F37626?style=for-the-badge&logo=jupyter&logoColor=white" />
        <img src="https://img.shields.io/badge/NumPy-013243?style=for-the-badge&logo=numpy&logoColor=white" />
      </td>
    </tr>
    <tr>
      <td align="center" width="140"><strong>Core Languages</strong></td>
      <td>
        <img src="https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white" />
        <img src="https://img.shields.io/badge/C++-00599C?style=for-the-badge&logo=cplusplus&logoColor=white" />
        <img src="https://img.shields.io/badge/Rust-000000?style=for-the-badge&logo=rust&logoColor=white" />
        <img src="https://img.shields.io/badge/TypeScript-3178C6?style=for-the-badge&logo=typescript&logoColor=white" />
      </td>
    </tr>
    <tr>
      <td align="center" width="140"><strong>Infrastructure</strong></td>
      <td>
        <img src="https://img.shields.io/badge/Linux-FCC624?style=for-the-badge&logo=linux&logoColor=black" />
        <img src="https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white" />
        <img src="https://img.shields.io/badge/Git-F05032?style=for-the-badge&logo=git&logoColor=white" />
        <img src="https://img.shields.io/badge/SQLite-003B57?style=for-the-badge&logo=sqlite&logoColor=white" />
      </td>
    </tr>
    <tr>
      <td align="center" width="140"><strong>Web Stack(I'm not happy about this, ngl)</strong></td>
      <td>
        <img src="https://img.shields.io/badge/React-61DAFB?style=for-the-badge&logo=react&logoColor=black" />
        <img src="https://img.shields.io/badge/Next.js-000000?style=for-the-badge&logo=nextdotjs&logoColor=white" />
        <img src="https://img.shields.io/badge/Tailwind-06B6D4?style=for-the-badge&logo=tailwindcss&logoColor=white" />
        <img src="https://img.shields.io/badge/FastAPI-009688?style=for-the-badge&logo=fastapi&logoColor=white" />
      </td>
    </tr>
  </table>
</div>

---

## Current Research & Learning Path

<table>
  <tr>
    <td width="33%" valign="top">
      <h3>Active Implementation</h3>
      <ul>
        <li><strong>State-Space Models</strong><br/>
        Mamba & RWKV from-scratch builds</li>
        <li><strong>Flash Attention v2</strong><br/>
        CUDA kernel-level understanding</li>
        <li><strong>Hybrid RNN-Attention</strong><br/>
        Exploring architecture combinations</li>
      </ul>
    </td>
    <td width="33%" valign="top">
      <h3>Next Phase (Q2 2025)</h3>
      <ul>
        <li><strong>CUDA Optimization</strong><br/>
        Custom kernels for inference</li>
        <li><strong>JAX Deep Dive</strong><br/>
        Functional programming for ML</li>
        <li><strong>Quantization Research</strong><br/>
        INT4/INT8 with minimal loss</li>
        <li><strong>Production Inference</strong><br/>
        Deployment-ready pipelines</li>
      </ul>
    </td>
    <td width="33%" valign="top">
      <h3>Philosophy</h3>
      <ul>
        <li><strong>Implementation First</strong><br/>
        Build it before believing it</li>
        <li><strong>Question Assumptions</strong><br/>
        Why transformers for everything?</li>
        <li><strong>Systematic Learning</strong><br/>
        No vibe coding, structured roadmap</li>
        <li><strong>Trade-off Analysis</strong><br/>
        Speed vs accuracy vs memory</li>
      </ul>
    </td>
  </tr>
</table>

---

## Collaboration & Contact

<div align="center">
  <p>
    <strong>Open to research collaboration & technical discussions</strong><br/>
    Interested in alternative architectures, efficient inference, or systematic ML learning?
  </p>

  <p>
    <strong>What I'm looking for:</strong><br/>
    • Co-researchers on non-transformer architectures<br/>
    • Code review & implementation feedback<br/>
    • Trade-off discussions: speed vs accuracy vs memory
  </p>

  <p>
    <strong>What I'm not interested in:</strong><br/>
    ❌ Wrapper apps without novel architecture<br/>
    ❌ "Just use ChatGPT API" projects<br/>
    ❌ Hype-driven development
  </p>

  <br/>

  <p>
    <a href="mailto:mehmetardahakbilen2005@gmail.com">
      <img src="https://img.shields.io/badge/Email-Research%20Inquiries-7f5af0?style=for-the-badge&logo=gmail&logoColor=white" />
    </a>
    <a href="https://www.linkedin.com/in/mehmet-arda-hakbilen-12aba6269/">
      <img src="https://img.shields.io/badge/LinkedIn-Connect-0a66c2?style=for-the-badge&logo=linkedin&logoColor=white" />
    </a>
    <a href="https://github.com/kaelvalen">
      <img src="https://img.shields.io/badge/GitHub-Follow-181717?style=for-the-badge&logo=github&logoColor=white" />
    </a>
  </p>

  <p>
    <em>"Implementation over theory. Trade-offs over hype. Systematic learning over vibe coding."</em>
  </p>

  <br/>

  <sub>
    Star repos if you find them useful | Building in public
  </sub>
</div>
